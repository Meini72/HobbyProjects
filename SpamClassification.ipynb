{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify an email to be spam or not, we have to convert the email into a feature vector firstly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Emails\n",
    "Browsing sample emails gives us a good feeling about how spam/non-spam emails may look like. \n",
    "Usually, the numbers, links and email addresses are different in almost every email. Therefore, \"normalizing\" these values may be a good idea, so that all the values would be treated in the same way. For example, we could replace each URL in the email with the string \"httpaddr\" to indicate a URL was present. This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn import svm\n",
    "import re\n",
    "import nltk, nltk.stem.porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample spam:\n",
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Sample spam:\")\n",
    "!cat data/emailSample1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preProcess(email):\n",
    "    email = email.lower() # make the email lower-case\n",
    "    email = re.sub('<[^<>]+>', ' ', email); # remove all the html tags\n",
    "    email = re.sub('[0-9]+', 'number', email); # replace all the specific numbers with general \"number\"\n",
    "    email = re.sub('(http|https)://[^\\s]*', 'httpaddr', email) # replace all the specific links with general \"httpaddr\"\n",
    "    email = re.sub('[^\\s]+@[^\\s]+', 'emailaddr', email) #replace all the specific email address with general \"emailaddr\"\n",
    "    email = re.sub('[$]+', 'dollar', email)\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email2wordlist(raw_email):\n",
    "    email = preProcess(raw_email)\n",
    "    lists = re.split('[ \\@\\$\\/\\#\\.\\-\\:\\&\\*\\+\\=\\[\\]\\?\\!\\(\\)\\{\\}\\,\\'\\\"\\>\\_\\<\\;\\%]', email)\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    wordList = []\n",
    "    for word in lists:\n",
    "        word = re.sub('[^a-zA-Z0-9]', '', word)\n",
    "        stemmed = stemmer.stem(word)\n",
    "        if not len(word): continue\n",
    "        wordList.append(stemmed)\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabDict():\n",
    "    vocab_dict = {}\n",
    "    with open('data/vocab.txt') as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            vocab_dict[key] = int(val)\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordlist2Indexlist(wordList, vocab_dict):\n",
    "    indexlist = []\n",
    "    indexlist = [vocab_dict[token] for token in wordList if token in vocab_dict]\n",
    "    return indexlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Vector Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexlist2FeatureVector(raw_email, vocab_dict):\n",
    "    feature_vector = np.zeros((len(vocab_dict),1))\n",
    "    wordList = email2wordlist(raw_email)\n",
    "    indexlist = wordlist2Indexlist(wordList, vocab_dict)\n",
    "    for idx in indexlist:\n",
    "        feature_vector[idx] = 1\n",
    "    return feature_vector, wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the feature vector is 1899.\n",
      "The number of non-zero elements is 45.\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = vocabDict()\n",
    "raw_email = open('data/emailSample1.txt', 'r').read()\n",
    "feature_vector, wordList = indexlist2FeatureVector(raw_email, vocab_dict)\n",
    "\n",
    "print(\"The length of the feature vector is %d.\" %len(feature_vector))\n",
    "print(\"The number of non-zero elements is %d.\" %sum(feature_vector==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anyon',\n",
       " 'know',\n",
       " 'how',\n",
       " 'much',\n",
       " 'it',\n",
       " 'cost',\n",
       " 'to',\n",
       " 'host',\n",
       " 'a',\n",
       " 'web',\n",
       " 'portal',\n",
       " 'well',\n",
       " 'it',\n",
       " 'depend',\n",
       " 'on',\n",
       " 'how',\n",
       " 'mani',\n",
       " 'visitor',\n",
       " 'you',\n",
       " 're',\n",
       " 'expect',\n",
       " 'thi',\n",
       " 'can',\n",
       " 'be',\n",
       " 'anywher',\n",
       " 'from',\n",
       " 'less',\n",
       " 'than',\n",
       " 'number',\n",
       " 'buck',\n",
       " 'a',\n",
       " 'month',\n",
       " 'to',\n",
       " 'a',\n",
       " 'coupl',\n",
       " 'of',\n",
       " 'dollarnumb',\n",
       " 'you',\n",
       " 'should',\n",
       " 'checkout',\n",
       " 'httpaddr',\n",
       " 'or',\n",
       " 'perhap',\n",
       " 'amazon',\n",
       " 'ecnumb',\n",
       " 'if',\n",
       " 'your',\n",
       " 'run',\n",
       " 'someth',\n",
       " 'big',\n",
       " 'to',\n",
       " 'unsubscrib',\n",
       " 'yourself',\n",
       " 'from',\n",
       " 'thi',\n",
       " 'mail',\n",
       " 'list',\n",
       " 'send',\n",
       " 'an',\n",
       " 'email',\n",
       " 'to',\n",
       " 'emailaddr']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training SVM for Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load a preprocessed training dataset to train a SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/spamTrain.mat'\n",
    "data = scipy.io.loadmat(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', 'y', '__version__', '__globals__', 'X'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/spamTest.mat'\n",
    "data = scipy.io.loadmat(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', 'ytest', '__version__', '__globals__', 'Xtest'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest = data['Xtest']\n",
    "ytest = data['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svm = svm.SVC(C=0.1, kernel='linear')\n",
    "linear_svm.fit(X, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy = 0.998250\n"
     ]
    }
   ],
   "source": [
    "train_predict = linear_svm.predict(X).reshape((y.shape[0],1))\n",
    "train_accuracy = sum(train_predict==y)/y.shape[0]\n",
    "print ('Training set accuracy = %f'%train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.989000\n"
     ]
    }
   ],
   "source": [
    "test_predict = linear_svm.predict(Xtest).reshape((ytest.shape[0],1))\n",
    "test_accuracy = sum(test_predict==ytest)/ytest.shape[0]\n",
    "print ('Test set accuracy = %f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Top Predictors of Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vocabList():\n",
    "    vocab_list = {}\n",
    "    with open('data/vocab.txt') as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            vocab_list[int(val)] = key\n",
    "    return vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = vocabList()\n",
    "sorted_index = np.argsort(-linear_svm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15 most important words for spam:\n",
      "['otherwis', 'clearli', 'remot', 'gt', 'visa', 'base', 'doesn', 'wife', 'previous', 'player', 'mortgag', 'natur', 'll', 'futur', 'hot']\n"
     ]
    }
   ],
   "source": [
    "print(\"The 15 most important words for spam:\")\n",
    "print([vocab_list[x] for x in sorted_index[0][0:15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15 least important words for spam:\n",
      "['http', 'toll', 'xp', 'ratio', 'august', 'unsubscrib', 'useless', 'numberth', 'round', 'linux', 'datapow', 'wrong', 'urgent', 'that', 'spam']\n"
     ]
    }
   ],
   "source": [
    "print(\"The 15 least important words for spam:\")\n",
    "print([vocab_list[x] for x in sorted_index[0][-15:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use preprocessed vocabulary dictionary. In practice, we have to build the dictionary by ourselves using, for example, the library \"collections\" (Counter(), most_common()) to select out the most popular 1000 or 5000 words.\n",
    "Then, we split the data into training dataset (60%), CV dataset (20%), testing dataset (20%). The C value is trained with training dataset and applied on the CV dataset. The C value with best performance on the CV dataset will be selected and fit on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
